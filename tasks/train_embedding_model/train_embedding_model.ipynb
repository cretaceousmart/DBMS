{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Embedding Model\n",
    "\n",
    "#### Use root-interval as encoding method, fasttext as embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import sys\n",
    "sys.path.append('/app/')\n",
    "from pitchclass2vec import encoding, model\n",
    "from pitchclass2vec.pitchclass2vec import Pitchclass2VecModel\n",
    "\n",
    "from tasks.segmentation.data import BillboardDataset, SegmentationDataModule\n",
    "from tasks.segmentation.functional import LSTMBaselineModel\n",
    "\n",
    "import pitchclass2vec.model as model\n",
    "import pitchclass2vec.encoding as encoding\n",
    "from pitchclass2vec.data import ChocoDataModule\n",
    "\n",
    "from evaluate import load_pitchclass2vec_model\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "pl.seed_everything(seed=RANDOM_SEED)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python /app/train.py --seed 42 --choco /app/choco_dataset/v1.0.0/jams_in_24_chords/ --out /app/out/root_interval_best --encoding root-interval --model fasttext --max_epochs 10 --early_stop_patience -1 --batch_size 512 --context 5 --negative_sampling_k 20 --embedding_dim 100 --wandb_run_name root-interval-fasttext-with-Processed-ChoCo\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Config the embedding model train process\n",
    "train_args = {\n",
    "    'seed': 42,\n",
    "    'choco': \"/app/choco_dataset/v1.0.0/jams_in_24_chords/\", # path for Choco Dataset\n",
    "    'out': \"/app/out/root_interval_best\", # path for output embedding model\n",
    "    'encoding': \"root-interval\", # path for encoder\n",
    "    'model': \"fasttext\", # path for the definition of embedding model\n",
    "    \n",
    "    'max_epochs': 10,\n",
    "    'early_stop_patience': -1, # If there's no significant change on loss, then keep trainning for 2 more epochs.\n",
    "\n",
    "    \n",
    "    'batch_size': 512,\n",
    "    'context': 5,\n",
    "    'negative_sampling_k': 20,\n",
    "    'embedding_dim': 100,\n",
    "    \n",
    "    'wandb_run_name': \"root-interval-fasttext-with-Processed-ChoCo\"\n",
    "\n",
    "}\n",
    "\n",
    "# Auto generate a Linux command\n",
    "command_parts = [\"python /app/tasks/train_embedding_model/embedding_train.py\"]\n",
    "for arg, value in train_args.items():\n",
    "    command_parts.append(f\"--{arg} {value}\")\n",
    "\n",
    "command = \" \".join(command_parts)\n",
    "print(command)\n",
    "\n",
    "print(\"done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excecute the Linux command\n",
    "!{command}\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
