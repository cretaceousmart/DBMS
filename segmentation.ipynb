{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "from pitchclass2vec import encoding, model\n",
    "from pitchclass2vec.pitchclass2vec import Pitchclass2VecModel\n",
    "\n",
    "from tasks.segmentation.data import BillboardDataset, SegmentationDataModule\n",
    "from tasks.segmentation.functional import LSTMBaselineModel\n",
    "\n",
    "import pitchclass2vec.model as model\n",
    "import pitchclass2vec.encoding as encoding\n",
    "from pitchclass2vec.data import ChocoDataModule\n",
    "\n",
    "from evaluate import load_pitchclass2vec_model\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "pl.seed_everything(seed=RANDOM_SEED)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Embedding Model\n",
    "\n",
    "#### Use root-interval as encoding method, fasttext as embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python /app/train.py --choco /app/choco_dataset/v1.0.0/ --out /app/out --encoding root-interval --model fasttext --batch_size 512 --context 5 --negative_sampling_k 20 --embedding_dim 100 --seed 42 --max_epochs 10 --early_stop_patience -1 --wandb_run_name first_run_with_whole_ChocoDataSet\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Config the embedding model train process\n",
    "train_args = {\n",
    "    'choco': \"/app/choco_dataset/v1.0.0/\", # path for Choco Dataset\n",
    "    'out': \"/app/out\", # path for output embedding model\n",
    "    'encoding': \"root-interval\", # path for encoder\n",
    "    'model': \"fasttext\", # path for the definition of embedding model\n",
    "    \n",
    "    'batch_size': 512,\n",
    "    'context': 5,\n",
    "    'negative_sampling_k': 20,\n",
    "    'embedding_dim': 100,\n",
    "    'seed': 42,\n",
    "    'max_epochs': 10,\n",
    "    'early_stop_patience': -1, # If there's no significant change on loss, then keep trainning for 2 more epochs.\n",
    "    \n",
    "    'wandb_run_name': \"first_run_with_whole_ChocoDataSet\"\n",
    "\n",
    "}\n",
    "\n",
    "# Auto generate a Linux command\n",
    "command_parts = [\"python /app/train.py\"]\n",
    "for arg, value in train_args.items():\n",
    "    command_parts.append(f\"--{arg} {value}\")\n",
    "\n",
    "command = \" \".join(command_parts)\n",
    "print(command)\n",
    "\n",
    "print(\"done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcretaceousmart\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/wandb/run-20231102_175554-ifca5kqb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfirst_run_with_whole_ChocoDataSet\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/cretaceousmart/pitchclass2vec\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cretaceousmart/pitchclass2vec/runs/ifca5kqb\u001b[0m\n",
      "Jie Log: data_path: /app/choco_dataset/v1.0.0/jams\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:604: UserWarning: Checkpoint directory /app/out exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type         | Params\n",
      "-------------------------------------------\n",
      "0 | embedding | EmbeddingBag | 409 K \n",
      "-------------------------------------------\n",
      "409 K     Trainable params\n",
      "0         Non-trainable params\n",
      "409 K     Total params\n",
      "1.638     Total estimated model params size (MB)\n",
      "Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3704/3704 [00:56<00:00, 66.12it/s, loss=0.574]`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3704/3704 [00:56<00:00, 66.08it/s, loss=0.574]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/loss ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/loss 0.44706\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mfirst_run_with_whole_ChocoDataSet\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cretaceousmart/pitchclass2vec/runs/ifca5kqb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m\u001b[0m\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Run the Linux command\n",
    "!{command}\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "EXP = [\n",
    "    #(\"text\", \"fasttext\", \"out/fasttext_best/model.ckpt\"),\n",
    "    # (\"timed-root-interval\", \"emb-weighted-fasttext\", \"/app/out/rootinterval_best/model.ckpt\"),\n",
    "    #(\"rdf\", \"randomwalk-rdf2vec\", \"out/rdf2vec_best/model.ckpt\"),\n",
    "    (\"root-interval\", \"fasttext\", \"/app/out/first_run_with_whole_ChocoDataSet.ckpt\"),\n",
    "]\n",
    "\n",
    "  \n",
    "experiments_df = pd.DataFrame(columns=[\n",
    "    \"encoding\", \"model\", \"path\", \"test_p_precision\", \"test_p_recall\",  \"test_p_f1\",  \"test_under\",  \"test_over\",  \"test_under_over_f1\"\n",
    "])\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e9b010d95a4d299fd3de12833c71c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcretaceousmart\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20231102_203138-ol56y5wq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cretaceousmart/pitchclass2vec_Segmentation/runs/ol56y5wq' target=\"_blank\">first_run.ckpt</a></strong> to <a href='https://wandb.ai/cretaceousmart/pitchclass2vec_Segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cretaceousmart/pitchclass2vec_Segmentation' target=\"_blank\">https://wandb.ai/cretaceousmart/pitchclass2vec_Segmentation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cretaceousmart/pitchclass2vec_Segmentation/runs/ol56y5wq' target=\"_blank\">https://wandb.ai/cretaceousmart/pitchclass2vec_Segmentation/runs/ol56y5wq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track 974 not parsable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 890/890 [00:01<00:00, 780.77it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "prefetch_factor option could only be specified in multiprocessing.let num_workers > 0 to enable multiprocessing.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/app/segmentation.ipynb Cell 8\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f63686f72642d656d62656464696e67732d6e65772d776974682d6a7570797465722d636f6e7461696e6572227d/app/segmentation.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m callbacks \u001b[39m=\u001b[39m [\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f63686f72642d656d62656464696e67732d6e65772d776974682d6a7570797465722d636f6e7461696e6572227d/app/segmentation.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m     pl\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mModelCheckpoint(save_top_k\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f63686f72642d656d62656464696e67732d6e65772d776974682d6a7570797465722d636f6e7461696e6572227d/app/segmentation.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m                                 monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain/loss\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f63686f72642d656d62656464696e67732d6e65772d776974682d6a7570797465722d636f6e7461696e6572227d/app/segmentation.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m                                 every_n_epochs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f63686f72642d656d62656464696e67732d6e65772d776974682d6a7570797465722d636f6e7461696e6572227d/app/segmentation.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m ] \n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f63686f72642d656d62656464696e67732d6e65772d776974682d6a7570797465722d636f6e7461696e6572227d/app/segmentation.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(max_epochs\u001b[39m=\u001b[39m\u001b[39m150\u001b[39m, \n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f63686f72642d656d62656464696e67732d6e65772d776974682d6a7570797465722d636f6e7461696e6572227d/app/segmentation.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m                      accelerator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f63686f72642d656d62656464696e67732d6e65772d776974682d6a7570797465722d636f6e7461696e6572227d/app/segmentation.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m                      devices\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f63686f72642d656d62656464696e67732d6e65772d776974682d6a7570797465722d636f6e7461696e6572227d/app/segmentation.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m                      enable_progress_bar\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f63686f72642d656d62656464696e67732d6e65772d776974682d6a7570797465722d636f6e7461696e6572227d/app/segmentation.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m                      callbacks\u001b[39m=\u001b[39mcallbacks)\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f63686f72642d656d62656464696e67732d6e65772d776974682d6a7570797465722d636f6e7461696e6572227d/app/segmentation.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(lstm_model, data)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f63686f72642d656d62656464696e67732d6e65772d776974682d6a7570797465722d636f6e7461696e6572227d/app/segmentation.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39m# wandb.save(str(Path(segmentation_train_args.get(\"segmentation_out\")) / f\"{segmentation_train_args.get('wandb_run_name')}.ckpt\"))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f63686f72642d656d62656464696e67732d6e65772d776974682d6a7570797465722d636f6e7461696e6572227d/app/segmentation.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m wandb\u001b[39m.\u001b[39msave(\u001b[39mstr\u001b[39m(Path(out) \u001b[39m/\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfile_name\u001b[39m}\u001b[39;00m\u001b[39m.ckpt\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py:603\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`Trainer.fit()` requires a `LightningModule`, got: \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    602\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 603\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    604\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    605\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[39m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py:645\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    638\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    639\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    640\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    641\u001b[0m     ckpt_path,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    643\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    644\u001b[0m )\n\u001b[0;32m--> 645\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    647\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    648\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py:1098\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1096\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1098\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1100\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1101\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py:1177\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1176\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1177\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py:1190\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_training_routine()\n\u001b[1;32m   1189\u001b[0m \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   1192\u001b[0m \u001b[39m# enable train mode\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py:1255\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1254\u001b[0m \u001b[39m# reload dataloaders\u001b[39;00m\n\u001b[0;32m-> 1255\u001b[0m val_loop\u001b[39m.\u001b[39;49m_reload_evaluation_dataloaders()\n\u001b[1;32m   1256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_sanity_val_batches \u001b[39m=\u001b[39m [\n\u001b[1;32m   1257\u001b[0m     \u001b[39mmin\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_sanity_val_steps, val_batches) \u001b[39mfor\u001b[39;00m val_batches \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_val_batches\n\u001b[1;32m   1258\u001b[0m ]\n\u001b[1;32m   1260\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:234\u001b[0m, in \u001b[0;36mEvaluationLoop._reload_evaluation_dataloaders\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    232\u001b[0m     dataloaders \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtest_dataloaders\n\u001b[1;32m    233\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mval_dataloaders \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39m_should_reload_val_dl:\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mreset_val_dataloader()\n\u001b[1;32m    235\u001b[0m     dataloaders \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mval_dataloaders\n\u001b[1;32m    236\u001b[0m \u001b[39mif\u001b[39;00m dataloaders \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py:1635\u001b[0m, in \u001b[0;36mTrainer.reset_val_dataloader\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m   1629\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39m==\u001b[39m TrainerFn\u001b[39m.\u001b[39mFITTING \u001b[39mand\u001b[39;00m (\n\u001b[1;32m   1630\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msanity_checking \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mepoch_loop\u001b[39m.\u001b[39m_should_check_val_epoch())\n\u001b[1;32m   1631\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msanity_checking\n\u001b[1;32m   1632\u001b[0m ):\n\u001b[1;32m   1633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_val_dl_reload_epoch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_epoch\n\u001b[0;32m-> 1635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_val_batches, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_dataloaders \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_connector\u001b[39m.\u001b[39;49m_reset_eval_dataloader(\n\u001b[1;32m   1636\u001b[0m     RunningStage\u001b[39m.\u001b[39;49mVALIDATING, model\u001b[39m=\u001b[39;49mpl_module\n\u001b[1;32m   1637\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:357\u001b[0m, in \u001b[0;36mDataConnector._reset_eval_dataloader\u001b[0;34m(self, mode, model)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39massert\u001b[39;00m mode\u001b[39m.\u001b[39mevaluating \u001b[39mor\u001b[39;00m mode \u001b[39m==\u001b[39m RunningStage\u001b[39m.\u001b[39mPREDICTING\n\u001b[1;32m    356\u001b[0m \u001b[39m# always get the loaders first so we can count how many there are\u001b[39;00m\n\u001b[0;32m--> 357\u001b[0m dataloaders \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request_dataloader(mode)\n\u001b[1;32m    359\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moverfit_batches \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    360\u001b[0m     dataloaders \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_resolve_overfit_batches(dataloaders, mode)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:446\u001b[0m, in \u001b[0;36mDataConnector._request_dataloader\u001b[0;34m(self, stage)\u001b[0m\n\u001b[1;32m    439\u001b[0m source \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mstage\u001b[39m.\u001b[39mdataloader_prefix\u001b[39m}\u001b[39;00m\u001b[39m_dataloader_source\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    441\u001b[0m \u001b[39mwith\u001b[39;00m _replace_dunder_methods(DataLoader, \u001b[39m\"\u001b[39m\u001b[39mdataset\u001b[39m\u001b[39m\"\u001b[39m), _replace_dunder_methods(BatchSampler):\n\u001b[1;32m    442\u001b[0m     \u001b[39m# under this context manager, the arguments passed to `DataLoader.__init__` will be captured and saved as\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[39m# attributes on the instance in case the dataloader needs to be re-instantiated later by Lightning.\u001b[39;00m\n\u001b[1;32m    444\u001b[0m     \u001b[39m# Also, it records all attribute setting and deletion using patched `__setattr__` and `__delattr__`\u001b[39;00m\n\u001b[1;32m    445\u001b[0m     \u001b[39m# methods so that the re-instantiated object is as close to the original as possible.\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     dataloader \u001b[39m=\u001b[39m source\u001b[39m.\u001b[39;49mdataloader()\n\u001b[1;32m    447\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(dataloader, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    448\u001b[0m     dataloader \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(dataloader)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:524\u001b[0m, in \u001b[0;36m_DataLoaderSource.dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstance, pl\u001b[39m.\u001b[39mLightningDataModule):\n\u001b[1;32m    523\u001b[0m     method \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstance, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[0;32m--> 524\u001b[0m     \u001b[39mreturn\u001b[39;00m method()\n\u001b[1;32m    526\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstance \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstance\n",
      "File \u001b[0;32m/app/tasks/segmentation/data.py:244\u001b[0m, in \u001b[0;36mSegmentationDataModule.val_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mval_dataloader\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataLoader:\n\u001b[1;32m    240\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[39m  Returns:\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[39m      DataLoader: DataLoader with validation data\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild_dataloader(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalid_dataset, shuffle\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m/app/tasks/segmentation/data.py:221\u001b[0m, in \u001b[0;36mSegmentationDataModule.build_dataloader\u001b[0;34m(self, dataset, shuffle)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_dataloader\u001b[39m(\u001b[39mself\u001b[39m, dataset: Dataset, shuffle: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataLoader:\n\u001b[1;32m    212\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[39m      dataset (Dataset): Dataset used in the dataloader.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[39m      DataLoader: Dataloader built using the specified dataset.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m   \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mDataLoader(\n\u001b[1;32m    222\u001b[0m     dataset,\n\u001b[1;32m    223\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size,\n\u001b[1;32m    224\u001b[0m     \u001b[39m# num_workers=os.cpu_count(),\u001b[39;49;00m\n\u001b[1;32m    225\u001b[0m     num_workers \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[1;32m    226\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m    227\u001b[0m     collate_fn\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset_cls\u001b[39m.\u001b[39;49mcollate_fn,\n\u001b[1;32m    228\u001b[0m     persistent_workers\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    229\u001b[0m     prefetch_factor\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m\n\u001b[1;32m    230\u001b[0m   )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/lightning_lite/utilities/data.py:323\u001b[0m, in \u001b[0;36m_wrap_init_method.<locals>.wrapper\u001b[0;34m(obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[39melif\u001b[39;00m store_explicit_arg \u001b[39min\u001b[39;00m kwargs:\n\u001b[1;32m    321\u001b[0m         \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(obj, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m__\u001b[39m\u001b[39m{\u001b[39;00mstore_explicit_arg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, kwargs[store_explicit_arg])\n\u001b[0;32m--> 323\u001b[0m init(obj, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    324\u001b[0m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m__pl_inside_init\u001b[39m\u001b[39m\"\u001b[39m, old_inside_init)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:241\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mtimeout option should be non-negative\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    240\u001b[0m \u001b[39mif\u001b[39;00m num_workers \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m prefetch_factor \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mprefetch_factor option could only be specified in multiprocessing.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    242\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mlet num_workers > 0 to enable multiprocessing.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    243\u001b[0m \u001b[39massert\u001b[39;00m prefetch_factor \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    245\u001b[0m \u001b[39mif\u001b[39;00m persistent_workers \u001b[39mand\u001b[39;00m num_workers \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: prefetch_factor option could only be specified in multiprocessing.let num_workers > 0 to enable multiprocessing."
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "logging.disable(logging.CRITICAL)\n",
    "\n",
    "segmentation_train_args = {\n",
    "    \"test_mode\" : False, # If test_mode = true, then we use 3 track for test\n",
    "    # \"segmentation_out\" : \"/app/segmentation_out\",\n",
    "    \"wandb_run_name\" : \"first_run.ckpt\",\n",
    "    \"disable_wandb\" : False,\n",
    "    \"num_labels\" : 11,\n",
    "    \"embedding_dim\" : None,  # Default as NoneÔºåwill use p2v.vector_size\n",
    "    \"hidden_size\" : 256,\n",
    "    \"num_layers\" : 5,\n",
    "    \"dropout\" : 0.2,\n",
    "    \"learning_rate\" : 0.001,\n",
    "}\n",
    "\n",
    "\n",
    "out = \"/app/segmentation_out\"\n",
    "file_name = \"first_segmentation_model\"\n",
    "\n",
    "for exp in tqdm(EXP):    \n",
    "    p2v = load_pitchclass2vec_model(*exp)\n",
    "    data = SegmentationDataModule(  dataset_cls=BillboardDataset, \n",
    "                                    pitchclass2vec=p2v, \n",
    "                                    batch_size = 256, \n",
    "                                    test_mode = segmentation_train_args.get(\"test_mode\", True)\n",
    "                                    )\n",
    "      \n",
    "    # lstm_model = LSTMBaselineModel(embedding_dim=p2v.vector_size, hidden_size=256, num_layers=5, dropout=0.2, learning_rate=0.001)\n",
    "    # TODO: figure out whether num_labels should be 8 or 11 (paper said 11 that could be correct, because 8 is not something from Billboard dataset but ChoCo dataset)\n",
    "    \n",
    "    lstm_model = LSTMBaselineModel(\n",
    "        num_labels=segmentation_train_args[\"num_labels\"],\n",
    "        embedding_dim=p2v.vector_size,\n",
    "        hidden_size=segmentation_train_args[\"hidden_size\"],\n",
    "        num_layers=segmentation_train_args[\"num_layers\"],\n",
    "        dropout=segmentation_train_args[\"dropout\"],\n",
    "        learning_rate=segmentation_train_args[\"learning_rate\"]\n",
    "    )\n",
    "\n",
    "    \n",
    "    if not segmentation_train_args.get(\"disable_wandb\", False):\n",
    "\n",
    "        wandb.init(\n",
    "            # Set the project where this run will be logged\n",
    "            project=\"pitchclass2vec_Segmentation\", \n",
    "            name=f\"{ segmentation_train_args.get('wandb_run_name', 'None') }\",\n",
    "            \n",
    "            # # Track hyperparameters and run metadata\n",
    "            config={\n",
    "                # Add any other parameters you want to track\n",
    "                \"num_labels\": segmentation_train_args[\"num_labels\"],\n",
    "                \"embedding_dim\": segmentation_train_args[\"embedding_dim\"] or p2v.vector_size,\n",
    "                \"hidden_size\": segmentation_train_args[\"hidden_size\"],\n",
    "                \"num_layers\": segmentation_train_args[\"num_layers\"],\n",
    "                \"dropout\": segmentation_train_args[\"dropout\"],\n",
    "                \"learning_rate\": segmentation_train_args[\"learning_rate\"],\n",
    "            }\n",
    "        )\n",
    "        wandb.watch(lstm_model)\n",
    "\n",
    "    file_name = f\"{segmentation_train_args.get('wandb_run_name')}\"\n",
    "    callbacks = [\n",
    "        pl.callbacks.ModelCheckpoint(save_top_k=1,\n",
    "                                    monitor=\"train/loss\",\n",
    "                                    mode=\"min\",\n",
    "                                    dirpath=out,\n",
    "                                    filename=file_name,\n",
    "                                    every_n_epochs=1)\n",
    "    ] \n",
    "    \n",
    "    trainer = pl.Trainer(max_epochs=150, \n",
    "                         accelerator=\"auto\", \n",
    "                         devices=1,\n",
    "                         enable_progress_bar=False,\n",
    "                         callbacks=callbacks)\n",
    "    \n",
    "    trainer.fit(lstm_model, data)\n",
    "\n",
    "    # wandb.save(str(Path(segmentation_train_args.get(\"segmentation_out\")) / f\"{segmentation_train_args.get('wandb_run_name')}.ckpt\"))\n",
    "    wandb.save(str(Path(out) / f\"{file_name}.ckpt\"))\n",
    "\n",
    "    test_metrics = trainer.test(lstm_model, data)\n",
    "    # Use pd.concat instead of pd.append\n",
    "    new_row_df = pd.DataFrame([{\n",
    "        \"encoding\": exp[0], \"model\": exp[1], \"path\": exp[2], **test_metrics[0]\n",
    "    }])\n",
    "    experiments_df = pd.concat([experiments_df, new_row_df], ignore_index=True)\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoding</th>\n",
       "      <th>model</th>\n",
       "      <th>path</th>\n",
       "      <th>test_p_precision</th>\n",
       "      <th>test_p_recall</th>\n",
       "      <th>test_p_f1</th>\n",
       "      <th>test_under</th>\n",
       "      <th>test_over</th>\n",
       "      <th>test_under_over_f1</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>root-interval</td>\n",
       "      <td>fasttext</td>\n",
       "      <td>/app/out/first_run_with_whole_ChocoDataSet.ckpt</td>\n",
       "      <td>0.424819</td>\n",
       "      <td>0.575204</td>\n",
       "      <td>0.488704</td>\n",
       "      <td>0.473333</td>\n",
       "      <td>0.637517</td>\n",
       "      <td>0.543292</td>\n",
       "      <td>1.457962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        encoding     model                                             path  \\\n",
       "0  root-interval  fasttext  /app/out/first_run_with_whole_ChocoDataSet.ckpt   \n",
       "\n",
       "   test_p_precision  test_p_recall  test_p_f1  test_under  test_over  \\\n",
       "0          0.424819       0.575204   0.488704    0.473333   0.637517   \n",
       "\n",
       "   test_under_over_f1  train_loss  \n",
       "0            0.543292    1.457962  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments_df"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1a065321f2d93fd20f60a317fd78c7406ef3cd4260c4249e728d2806e6b26b96"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
