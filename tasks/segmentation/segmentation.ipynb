{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import sys \n",
    "sys.path.append('/app/')\n",
    "RANDOM_SEED = 42\n",
    "pl.seed_everything(seed=RANDOM_SEED)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Segmentation model (Transformer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import torch\n",
    "logging.disable(logging.CRITICAL)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "exp_args = {\n",
    "    \"encoder\": \"root-interval\",\n",
    "}\n",
    "\n",
    "segmentation_train_args = {\n",
    "    \"seed\": 42,\n",
    "    # \"test_mode\": False,\n",
    "    # \"full_chord\": False,\n",
    "    # \"disable_wandb\": False,\n",
    "    \"wandb_run_name\": \"transformer_test_run\",\n",
    "    \"out\": \"/app/segmentation_out\",\n",
    "\n",
    "    \"input_dim\": 3,  # å‡è®¾å’Œå¼¦åºåˆ—ç»è¿‡ç¼–ç åçš„ç‰¹å¾ç»´åº¦ä¸º3\n",
    "    \"model_dim\": 128,  # Transformer å†…éƒ¨ä½¿ç”¨çš„éšè—å±‚ç»´åº¦\n",
    "    \"feedforward_dim\": 256,  # Transformer çš„å‰é¦ˆç½‘ç»œç»´åº¦\n",
    "    \"num_classes\": 14,  # é¢„æµ‹çš„ç±»åˆ«æ•°ï¼ˆsection label çš„ç±»åˆ«æ•°ï¼‰\n",
    "    \"num_heads\": 3,  # å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¸­å¤´çš„æ•°é‡\n",
    "    \"num_layers\": 6,  # Transformer æ¨¡å‹ä¸­ç¼–ç å™¨å’Œè§£ç å™¨çš„å±‚æ•°\n",
    "    \"decoder_max_length\": 500,  # è§£ç å™¨çš„æœ€å¤§åºåˆ—é•¿åº¦\n",
    "    \"init_method\": \"xavier\", # or orthogonal\n",
    "\n",
    "    \"max_epochs\":1,\n",
    "    \"batch_size\": 128,\n",
    "    \"device\": device,  # è®­ç»ƒæ¨¡å‹ä½¿ç”¨çš„è®¾å¤‡\n",
    "    \"lr\": 0.001,  # å­¦ä¹ ç‡\n",
    "    \"warmup\": 100,  # å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°\n",
    "    \"factor\": 0.5,\n",
    "    \"patience\": 5,\n",
    "    \"max_iters\": 1,  # è®­ç»ƒçš„æœ€å¤§è¿­ä»£æ¬¡æ•°\n",
    "    \"dropout\": 0.1,  # åœ¨æ¨¡å‹ä¸­åº”ç”¨çš„ dropout æ¯”ç‡\n",
    "    \"input_dropout\": 0.1,  # åœ¨è¾“å…¥ç‰¹å¾ä¸Šåº”ç”¨çš„ dropout æ¯”ç‡\n",
    "}\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python /app/tasks/segmentation/trainning_function/transformer_train.py --encoder root-interval --seed 42 --wandb_run_name transformer_test_run --out /app/segmentation_out --input_dim 3 --model_dim 128 --feedforward_dim 256 --num_classes 14 --num_heads 3 --num_layers 6 --decoder_max_length 500 --init_method xavier --max_epochs 1 --batch_size 128 --device cuda --lr 0.001 --warmup 100 --factor 0.5 --patience 5 --max_iters 1 --dropout 0.1 --input_dropout 0.1\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Auto generate a Linux command\n",
    "command_parts = [\"python /app/tasks/segmentation/trainning_function/transformer_train.py\"]\n",
    "\n",
    "for arg, value in exp_args.items():\n",
    "    command_parts.append(f\"--{arg} {value}\")\n",
    "\n",
    "for arg, value in segmentation_train_args.items():\n",
    "    command_parts.append(f\"--{arg} {value}\")\n",
    "\n",
    "command = \" \".join(command_parts)\n",
    "print(command)\n",
    "\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcretaceousmart\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/app/tasks/segmentation/wandb/run-20231122_232627-yddio2qg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtransformer_test_run\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cretaceousmart/Segmentation_with_Transformer\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cretaceousmart/Segmentation_with_Transformer/runs/yddio2qg\u001b[0m\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹          | 661/890 [00:00<00:00, 818.02it/s]Track 974 not parsable\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890/890 [00:01<00:00, 799.25it/s]\n",
      "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:604: UserWarning: Checkpoint directory /app/segmentation_out exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "Sanity Checking DataLoader 0:   0%|                       | 0/1 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"/app/tasks/segmentation/trainning_function/transformer_train.py\", line 208, in <module>\n",
      "    experiments_df = train(exp_args, segmentation_train_args)\n",
      "  File \"/app/tasks/segmentation/trainning_function/transformer_train.py\", line 110, in train\n",
      "    trainer.fit(transformer_model, data)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 603, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/call.py\", line 38, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 645, in _fit_impl\n",
      "    self._run(model, ckpt_path=self.ckpt_path)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1098, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1177, in _run_stage\n",
      "    self._run_train()\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1190, in _run_train\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1262, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/loop.py\", line 199, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 152, in advance\n",
      "    dl_outputs = self.epoch_loop.run(self._data_fetcher, dl_max_batches, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/loop.py\", line 199, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 137, in advance\n",
      "    output = self._evaluation_step(**kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 234, in _evaluation_step\n",
      "    output = self.trainer._call_strategy_hook(hook_name, *kwargs.values())\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1480, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/strategies/strategy.py\", line 390, in validation_step\n",
      "    return self.model.validation_step(*args, **kwargs)\n",
      "  File \"/app/tasks/segmentation/deeplearning_models/base.py\", line 103, in validation_step\n",
      "    loss, metrics = self._test(batch)\n",
      "  File \"/app/tasks/segmentation/deeplearning_models/base.py\", line 45, in _test\n",
      "    pred, loss = self._predict(batch)\n",
      "  File \"/app/tasks/segmentation/deeplearning_models/transformer.py\", line 403, in _predict\n",
      "    source = self.positional_encoding(source)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/tasks/segmentation/deeplearning_models/transformer.py\", line 45, in forward\n",
      "    x = x + self.pe[:, : x.size(1)]\n",
      "RuntimeError: The size of tensor a (3) must match the size of tensor b (128) at non-singleton dimension 2\n",
      "Traceback (most recent call last):\n",
      "  File \"/app/tasks/segmentation/trainning_function/transformer_train.py\", line 208, in <module>\n",
      "    experiments_df = train(exp_args, segmentation_train_args)\n",
      "  File \"/app/tasks/segmentation/trainning_function/transformer_train.py\", line 110, in train\n",
      "    trainer.fit(transformer_model, data)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 603, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/call.py\", line 38, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 645, in _fit_impl\n",
      "    self._run(model, ckpt_path=self.ckpt_path)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1098, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1177, in _run_stage\n",
      "    self._run_train()\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1190, in _run_train\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1262, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/loop.py\", line 199, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 152, in advance\n",
      "    dl_outputs = self.epoch_loop.run(self._data_fetcher, dl_max_batches, kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/loop.py\", line 199, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 137, in advance\n",
      "    output = self._evaluation_step(**kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 234, in _evaluation_step\n",
      "    output = self.trainer._call_strategy_hook(hook_name, *kwargs.values())\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1480, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pytorch_lightning/strategies/strategy.py\", line 390, in validation_step\n",
      "    return self.model.validation_step(*args, **kwargs)\n",
      "  File \"/app/tasks/segmentation/deeplearning_models/base.py\", line 103, in validation_step\n",
      "    loss, metrics = self._test(batch)\n",
      "  File \"/app/tasks/segmentation/deeplearning_models/base.py\", line 45, in _test\n",
      "    pred, loss = self._predict(batch)\n",
      "  File \"/app/tasks/segmentation/deeplearning_models/transformer.py\", line 403, in _predict\n",
      "    source = self.positional_encoding(source)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/app/tasks/segmentation/deeplearning_models/transformer.py\", line 45, in forward\n",
      "    x = x + self.pe[:, : x.size(1)]\n",
      "RuntimeError: The size of tensor a (3) must match the size of tensor b (128) at non-singleton dimension 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run \u001b[33mtransformer_test_run\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cretaceousmart/Segmentation_with_Transformer/runs/yddio2qg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ï¸âš¡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/cretaceousmart/Segmentation_with_Transformer/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExNzk2NjMwMg==/version_details/v0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231122_232627-yddio2qg/logs\u001b[0m\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Excecute the Linux command\n",
    "!{command}\n",
    "print(\"done!\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1a065321f2d93fd20f60a317fd78c7406ef3cd4260c4249e728d2806e6b26b96"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
