{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Madmom for Chords Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install python libray (TODO: Add to requirement.txt for Docker)\n",
    "\n",
    "# pip install pandas==1.3.5\n",
    "# pip install numpy==1.19.5\n",
    "# pip install scipy==1.10\n",
    "# pip install matplotlib==3.6\n",
    "# pip install madmom\n",
    "\n",
    "# install ffmpeg on Ubuntu\n",
    "# apt update\n",
    "# apt install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/app')\n",
    "from evaluate import load_pitchclass2vec_model\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import groupby\n",
    "import re\n",
    "\n",
    "print(\"done\")\n",
    "RANDOM_SEED = 42\n",
    "pl.seed_everything(seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 0.00, End: 4.30, Chord: E:maj\n",
      "Start: 4.30, End: 7.50, Chord: C#:min\n",
      "Start: 7.50, End: 10.60, Chord: F#:maj\n",
      "Start: 10.60, End: 11.70, Chord: D#:min\n",
      "Start: 11.70, End: 13.00, Chord: B:maj\n",
      "Start: 13.00, End: 13.70, Chord: G#:min\n",
      "Start: 13.70, End: 14.10, Chord: D#:min\n",
      "Start: 14.10, End: 15.20, Chord: F#:maj\n",
      "Start: 15.20, End: 21.70, Chord: E:maj\n",
      "Start: 21.70, End: 24.90, Chord: C#:min\n",
      "Start: 24.90, End: 25.60, Chord: E:maj\n",
      "Start: 25.60, End: 26.20, Chord: D#:min\n",
      "Start: 26.20, End: 30.20, Chord: B:maj\n",
      "Start: 30.20, End: 31.00, Chord: D#:min\n",
      "Start: 31.00, End: 32.20, Chord: F#:maj\n",
      "Start: 32.20, End: 38.70, Chord: E:maj\n",
      "Start: 38.70, End: 41.20, Chord: N\n",
      "Start: 41.20, End: 42.50, Chord: A:maj\n",
      "Start: 42.50, End: 47.30, Chord: B:maj\n",
      "Start: 47.30, End: 48.70, Chord: G#:min\n",
      "Start: 48.70, End: 49.50, Chord: F#:maj\n",
      "Start: 49.50, End: 50.70, Chord: N\n",
      "Start: 50.70, End: 53.60, Chord: E:maj\n",
      "Start: 53.60, End: 54.70, Chord: C:maj\n",
      "Start: 54.70, End: 56.10, Chord: E:maj\n",
      "Start: 56.10, End: 58.20, Chord: N\n",
      "Start: 58.20, End: 59.00, Chord: B:maj\n",
      "Start: 59.00, End: 63.30, Chord: E:maj\n",
      "Start: 63.30, End: 65.30, Chord: C#:min\n",
      "Start: 65.30, End: 66.70, Chord: C#:maj\n",
      "Start: 66.70, End: 68.50, Chord: F#:maj\n",
      "Start: 68.50, End: 69.90, Chord: B:maj\n",
      "Start: 69.90, End: 71.70, Chord: D#:min\n",
      "Start: 71.70, End: 72.90, Chord: G#:min\n",
      "Start: 72.90, End: 74.10, Chord: F#:maj\n",
      "Start: 74.10, End: 80.40, Chord: E:maj\n",
      "Start: 80.40, End: 83.80, Chord: C#:min\n",
      "Start: 83.80, End: 89.10, Chord: B:maj\n",
      "Start: 89.10, End: 91.20, Chord: F#:maj\n",
      "Start: 91.20, End: 97.70, Chord: E:maj\n",
      "Start: 97.70, End: 101.00, Chord: C#:min\n",
      "Start: 101.00, End: 101.90, Chord: F#:maj\n",
      "Start: 101.90, End: 102.50, Chord: D#:min\n",
      "Start: 102.50, End: 106.20, Chord: B:maj\n",
      "Start: 106.20, End: 108.30, Chord: F#:maj\n",
      "Start: 108.30, End: 114.90, Chord: E:maj\n",
      "Start: 114.90, End: 118.30, Chord: C#:min\n",
      "Start: 118.30, End: 118.90, Chord: B:maj\n",
      "Start: 118.90, End: 120.00, Chord: D#:min\n",
      "Start: 120.00, End: 124.20, Chord: B:maj\n",
      "Start: 124.20, End: 125.50, Chord: F#:maj\n",
      "Start: 125.50, End: 132.20, Chord: E:maj\n",
      "Start: 132.20, End: 135.30, Chord: C#:min\n",
      "Start: 135.30, End: 136.20, Chord: F#:maj\n",
      "Start: 136.20, End: 137.00, Chord: D#:min\n",
      "Start: 137.00, End: 140.90, Chord: B:maj\n",
      "Start: 140.90, End: 141.50, Chord: D#:min\n",
      "Start: 141.50, End: 142.60, Chord: F#:maj\n",
      "Start: 142.60, End: 148.90, Chord: E:maj\n",
      "Start: 148.90, End: 152.20, Chord: C#:min\n",
      "Start: 152.20, End: 153.10, Chord: F#:maj\n",
      "Start: 153.10, End: 156.60, Chord: D#:min\n",
      "Start: 156.60, End: 157.70, Chord: B:maj\n",
      "Start: 157.70, End: 159.60, Chord: G#:maj\n",
      "Start: 159.60, End: 166.20, Chord: E:maj\n",
      "Start: 166.20, End: 169.80, Chord: C#:min\n",
      "Start: 169.80, End: 174.90, Chord: B:maj\n",
      "Start: 174.90, End: 175.80, Chord: D#:min\n",
      "Start: 175.80, End: 176.90, Chord: F#:maj\n",
      "Start: 176.90, End: 184.40, Chord: E:maj\n",
      "Start: 184.40, End: 186.70, Chord: C#:min\n",
      "Start: 186.70, End: 187.50, Chord: F#:maj\n",
      "Start: 187.50, End: 188.30, Chord: D#:min\n",
      "Start: 188.30, End: 191.50, Chord: B:maj\n",
      "Start: 191.50, End: 192.40, Chord: D#:min\n",
      "Start: 192.40, End: 194.00, Chord: F#:maj\n",
      "Start: 194.00, End: 200.20, Chord: E:maj\n",
      "Start: 200.20, End: 203.90, Chord: C#:min\n",
      "Start: 203.90, End: 204.60, Chord: B:maj\n",
      "Start: 204.60, End: 205.80, Chord: D#:min\n",
      "Start: 205.80, End: 209.90, Chord: B:maj\n",
      "Start: 209.90, End: 211.20, Chord: F#:maj\n",
      "Start: 211.20, End: 218.00, Chord: E:maj\n",
      "Start: 218.00, End: 221.00, Chord: C#:min\n",
      "Start: 221.00, End: 221.80, Chord: F#:maj\n",
      "Start: 221.80, End: 222.80, Chord: D#:min\n",
      "Start: 222.80, End: 226.80, Chord: B:maj\n",
      "Start: 226.80, End: 228.30, Chord: F#:maj\n",
      "Start: 228.30, End: 239.10, Chord: E:maj\n",
      "Start: 239.10, End: 241.30, Chord: N\n",
      "Start: 241.30, End: 242.70, Chord: C#:maj\n",
      "Start: 242.70, End: 244.40, Chord: N\n",
      "Start: 244.40, End: 245.70, Chord: D#:maj\n",
      "Start: 245.70, End: 247.20, Chord: F#:maj\n",
      "Start: 247.20, End: 248.00, Chord: D#:maj\n",
      "Start: 248.00, End: 249.00, Chord: D#:min\n",
      "Start: 249.00, End: 250.20, Chord: B:maj\n",
      "Start: 250.20, End: 251.20, Chord: D#:min\n",
      "Start: 251.20, End: 253.40, Chord: D#:maj\n",
      "Start: 253.40, End: 254.20, Chord: B:maj\n",
      "Start: 254.20, End: 255.20, Chord: D#:maj\n",
      "Start: 255.20, End: 258.40, Chord: C#:maj\n",
      "Start: 258.40, End: 260.50, Chord: F#:maj\n",
      "Start: 260.50, End: 270.00, Chord: E:maj\n",
      "Start: 270.00, End: 272.40, Chord: C#:min\n",
      "Start: 272.40, End: 273.20, Chord: F#:maj\n",
      "Start: 273.20, End: 274.00, Chord: D#:min\n",
      "Start: 274.00, End: 277.60, Chord: B:maj\n",
      "Start: 277.60, End: 279.70, Chord: F#:maj\n",
      "Start: 279.70, End: 286.20, Chord: E:maj\n",
      "Start: 286.20, End: 289.40, Chord: C#:min\n",
      "Start: 289.40, End: 290.20, Chord: E:maj\n",
      "Start: 290.20, End: 291.00, Chord: D#:min\n",
      "Start: 291.00, End: 294.80, Chord: B:maj\n",
      "Start: 294.80, End: 296.20, Chord: D#:min\n",
      "Start: 296.20, End: 296.90, Chord: F#:maj\n",
      "Start: 296.90, End: 299.00, Chord: E:maj\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# 导入所需的模块\n",
    "import numpy as np\n",
    "from madmom.features.chords import CNNChordFeatureProcessor, CRFChordRecognitionProcessor\n",
    "from madmom.processors import SequentialProcessor\n",
    "\n",
    "# 音频文件路径\n",
    "audio_file = '/app/jie_test_music/Drake - Passionfruit.mp3'\n",
    "\n",
    "# 创建一个特征提取器实例\n",
    "feature_processor = CNNChordFeatureProcessor()\n",
    "\n",
    "# 创建一个和弦识别器实例\n",
    "chord_recognizer = CRFChordRecognitionProcessor()\n",
    "\n",
    "# 将两个处理器串联成一个序列处理器\n",
    "sequential_processor = SequentialProcessor([feature_processor, chord_recognizer])\n",
    "\n",
    "# 应用处理器到音频文件上，识别和弦\n",
    "chords = sequential_processor(audio_file)\n",
    "\n",
    "# 打印识别出的和弦\n",
    "for chord in chords:\n",
    "    start, end, label = chord\n",
    "    print(f\"Start: {start:.2f}, End: {end:.2f}, Chord: {label}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117\n",
      "<class 'numpy.ndarray'>\n",
      "[(0. ,  4.3, 'E:maj') (4.3,  7.5, 'C#:min') (7.5, 10.6, 'F#:maj')]\n"
     ]
    }
   ],
   "source": [
    "# print the info of chords array\n",
    "print(len(chords))\n",
    "print(type(chords))\n",
    "print(chords[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pitchclass2vec.pitchclass2vec import NaiveEmbeddingModel\n",
    "from pitchclass2vec import encoding, model\n",
    "\n",
    "encoder = encoding.RootIntervalDataset\n",
    "embedding_model = NaiveEmbeddingModel(\n",
    "                        encoding_model=encoder, \n",
    "                        embedding_dim=3, # dim=3 because each '24 basic chords' only contain 3 notes\n",
    "                        norm=False)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done: loaded the model\n",
      "corpus_embedded.shape: (117, 3)\n",
      "corpus_embedded.shape: torch.Size([1, 117, 3])\n",
      "Result after classification:tensor([[[-1.6412,  0.5245,  0.4142,  ..., -1.6219, -0.5268,  1.4434],\n",
      "         [-2.1164,  0.9851,  0.2581,  ..., -1.7306, -0.7539,  2.0001],\n",
      "         [-2.2960,  1.3868,  0.2926,  ..., -1.8313, -0.8097,  2.3418],\n",
      "         ...,\n",
      "         [ 1.4268,  2.9531,  2.0472,  ..., -2.0260,  0.0395,  2.1543],\n",
      "         [ 1.2329,  2.7068,  1.9630,  ..., -1.9274,  0.0187,  1.9478],\n",
      "         [ 1.0434,  2.2073,  1.7487,  ..., -1.6685,  0.0984,  1.5906]]],\n",
      "       device='cuda:0')\n",
      "Result after softmax: tensor([[[-1.6412,  0.5245,  0.4142,  ..., -1.6219, -0.5268,  1.4434],\n",
      "         [-2.1164,  0.9851,  0.2581,  ..., -1.7306, -0.7539,  2.0001],\n",
      "         [-2.2960,  1.3868,  0.2926,  ..., -1.8313, -0.8097,  2.3418],\n",
      "         ...,\n",
      "         [ 1.4268,  2.9531,  2.0472,  ..., -2.0260,  0.0395,  2.1543],\n",
      "         [ 1.2329,  2.7068,  1.9630,  ..., -1.9274,  0.0187,  1.9478],\n",
      "         [ 1.0434,  2.2073,  1.7487,  ..., -1.6685,  0.0984,  1.5906]]],\n",
      "       device='cuda:0')\n",
      "Prediction shape: torch.Size([1, 117, 11])\n"
     ]
    }
   ],
   "source": [
    "# from tasks.segmentation.functional import LSTMBaselineModel\n",
    "from tasks.segmentation.deeplearning_models.lstm import LSTMBaselineModel\n",
    "import torch \n",
    "\n",
    "# Embed the chord by embedding model (store in /out)\n",
    "def embed_chord(p2v, c):\n",
    "    try:\n",
    "        return p2v[c]\n",
    "    except:\n",
    "        return p2v[\"N\"]\n",
    "\n",
    "# Load the pre-trained embedding model\n",
    "# p2v = load_pitchclass2vec_model(\"root-interval\", \"fasttext\", \"/app/out/root_interval_best/root-interval-fasttext-with-Processed-ChoCo.ckpt\")\n",
    "print(\"done: loaded the model\")\n",
    "\n",
    "# Load the pre-trained LSTM model for prediction:\n",
    "CKPT_PATH = '/app/segmentation_out/13_run.ckpt'\n",
    "model = LSTMBaselineModel.load_from_checkpoint(CKPT_PATH)\n",
    "\n",
    "# Obtain the embedded chords \n",
    "chords_str = [chord[2] for chord in chords]\n",
    "corpus_embedded = np.stack([\n",
    "    np.mean(np.array([embed_chord(embedding_model, c) for c in x]), axis=0) for x in chords\n",
    "])\n",
    "\n",
    "print(f\"corpus_embedded.shape: {corpus_embedded.shape}\",end='\\n')\n",
    "\n",
    "\n",
    "# Obtain correct input format: add a batchsize on the first position\n",
    "corpus_embedded = torch.tensor(corpus_embedded).unsqueeze(0)\n",
    "print(f\"corpus_embedded.shape: {corpus_embedded.shape}\",end='\\n')\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Evalutaion mode and  Move to GPU\n",
    "model.eval() \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "corpus_embedded = corpus_embedded.float().to(device) # change float(float 64) into double(float32)\n",
    "\n",
    "\n",
    "\n",
    "# Prediction: will return (x,y), x is result after classification(x), y is x after softmax(x)\n",
    "with torch.no_grad():\n",
    "    predictions = model.evaluation_forward(corpus_embedded)\n",
    "\n",
    "print(f\"Prediction shape: {predictions[1].shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: chords_str: ['E:maj', 'C#:min', 'F#:maj', 'D#:min', 'B:maj', 'G#:min', 'D#:min', 'F#:maj', 'E:maj', 'C#:min', 'E:maj', 'D#:min', 'B:maj', 'D#:min', 'F#:maj', 'E:maj', 'N', 'A:maj', 'B:maj', 'G#:min', 'F#:maj', 'N', 'E:maj', 'C:maj', 'E:maj', 'N', 'B:maj', 'E:maj', 'C#:min', 'C#:maj', 'F#:maj', 'B:maj', 'D#:min', 'G#:min', 'F#:maj', 'E:maj', 'C#:min', 'B:maj', 'F#:maj', 'E:maj', 'C#:min', 'F#:maj', 'D#:min', 'B:maj', 'F#:maj', 'E:maj', 'C#:min', 'B:maj', 'D#:min', 'B:maj', 'F#:maj', 'E:maj', 'C#:min', 'F#:maj', 'D#:min', 'B:maj', 'D#:min', 'F#:maj', 'E:maj', 'C#:min', 'F#:maj', 'D#:min', 'B:maj', 'G#:maj', 'E:maj', 'C#:min', 'B:maj', 'D#:min', 'F#:maj', 'E:maj', 'C#:min', 'F#:maj', 'D#:min', 'B:maj', 'D#:min', 'F#:maj', 'E:maj', 'C#:min', 'B:maj', 'D#:min', 'B:maj', 'F#:maj', 'E:maj', 'C#:min', 'F#:maj', 'D#:min', 'B:maj', 'F#:maj', 'E:maj', 'N', 'C#:maj', 'N', 'D#:maj', 'F#:maj', 'D#:maj', 'D#:min', 'B:maj', 'D#:min', 'D#:maj', 'B:maj', 'D#:maj', 'C#:maj', 'F#:maj', 'E:maj', 'C#:min', 'F#:maj', 'D#:min', 'B:maj', 'F#:maj', 'E:maj', 'C#:min', 'E:maj', 'D#:min', 'B:maj', 'D#:min', 'F#:maj', 'E:maj']\n",
      "\n",
      "\n",
      "Output: predicted_labels: tensor([[ 4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4, 10, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "         10, 10, 10, 10, 10, 10, 10, 10, 10,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = torch.argmax(predictions[1], dim=-1)  # shape will be (1, 115)\n",
    "print(f\"Input: chords_str: {chords_str}\")\n",
    "print('\\n')\n",
    "print(f\"Output: predicted_labels: {predicted_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end_time: [32.2, 108.30000000000001]\n"
     ]
    }
   ],
   "source": [
    "label_list = list(predicted_labels)[0]\n",
    "change_point = []\n",
    "for i in range(len(label_list)-1):\n",
    "    if label_list[i] != label_list[i+1]: change_point.append(i)\n",
    "\n",
    "end_time = [chords[cp][1] for cp in change_point]\n",
    "print(f\"end_time: {end_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "predicted_indices = torch.argmax(predictions[1], dim=-1)\n",
    "\n",
    "# 将这些索引转换成独热编码格式\n",
    "predicted_onehot = torch.zeros_like(predictions[1]).scatter_(-1, predicted_indices.unsqueeze(-1), 1)\n",
    "\n",
    "# 转换为numpy数组\n",
    "predicted_onehot = predicted_onehot.cpu().numpy()\n",
    "\n",
    "# 使用inverse_transform将独热编码转换回原始标签\n",
    "label_encoder = \n",
    "predicted_labels = label_encoder.inverse_transform(predicted_onehot)\n",
    "\n",
    "print(f\"Predicted labels: {predicted_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
